{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Chapter 14.  Overview of the GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## -----------------------------------------------------------------------------------------------------------------------------\n",
    "## Contents\n",
    "### 14.1 The GLM\n",
    "### 14.2 Cases of the GLM\n",
    "## -----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1 The Generalized Linear Model(GLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.1 Predictor and predicted variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to predict someone’s weight from their height. \n",
    "> Predicted variable(독립변수): weight\n",
    "\n",
    "> Predictor(설명변수): height\n",
    "\n",
    "suppose we want to predict high school grade point average (GPA) from Scholastic Aptitude Test (SAT) score and family income.\n",
    "> Predicted variable(독립변수): GPA\n",
    "\n",
    "> Predictor(설명변수): SAT and income\n",
    "\n",
    "** The value of the predictor variable comes from\n",
    "“outside” the system being modeled, whereas the value of the predicted variable depends\n",
    "on the value of the predictor variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####The key mathematical difference between predictor and predicted variables is that the likelihood function.\n",
    "(Likelihood function: expresses the probability of values ofthe predicted variable as a function of values of the predictor variable. The likelihood function does not describe the probabilities of values of the predictor variable.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In experimental settings, the variables that are actually manipulated and set by the\n",
    "experimenter are the independent variables. In this context of experimental manipulation, the values of the independent variables truly are (in principle, at least) independent of the values of other variables, because the experimenter has intervened to arbitrarily set the values of the independent variables. But sometimes a non-manipulated variable is also referred to as “independent”, merely as a way to indicate that it is being used as a predictor variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among non-manipulated variables, the roles of predicted and predictor are arbitrary, determined only by the interpretation of the analysis. \n",
    "\n",
    "> Consider, for example, people’s weights and heights. We could be interested in predicting a person’s weight from his/her height, or we could be interested in predicting a person’s height from his/her weight.\n",
    "\n",
    "Prediction is merely a mathematical dependency, not necessarily a description of underlying causal relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as “prediction” does not imply causation, “prediction” also does not imply any temporal relation between the variables. \n",
    "\n",
    "> For example, we may want to predict a person’s\n",
    "sex, male or female, from his/her height. Because males tend to be taller than females, this\n",
    "prediction can be made with better than chance accuracy. But a person’s sex is not caused\n",
    "by his/her height, nor does a person’s sex occur only after their height is measured. \n",
    "\n",
    "Thus,\n",
    "we can “predict” a person’s sex from his/her height, but this does not mean that the person’s\n",
    "sex occurred later in time than his/her height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In summary: \n",
    "#### All manipulated independent variables are predictor variables, not predicted. \n",
    "####Some dependent variables can take on the role of predictor variables, if desired. All predicted variables are dependent variables. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we care.\n",
    "We care about these distinctions between predicted and predictor variables <B>because the likelihood function is a mathematical description of the dependency of the predicted variable on the predictor variable.</B>\n",
    "\n",
    "The first thing we have to do in statistical inference is identify what variables we are interested in predicting, on the basis of what\n",
    "predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.2 Scale types: metric, ordinal, nominal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Items can be measured on different scales. \n",
    "\n",
    "> For example, the participants in a foot race can be measured \n",
    ">> * Time they took to run the race. -> <B>metric</B>\n",
    ">> * Placing in the race (1st,2nd, 3rd, etc.) -> <B>ordinal</B>\n",
    ">> * the name of the team they represent. -> <B>nominal scales</B>\n",
    "\n",
    "\n",
    "Examples of <B>metric-scaled data</B>  include response time (i.e., latency or duration), temperature,\n",
    "height, and weight. \n",
    "\n",
    "> * <B>Ratio scale</B>, because they have a natural zero point on the scale. \n",
    "> * <B>Interval scales</B>, because all we know about them is the amount of stuff in an interval on the scale, not the amount of stuff at a\n",
    "point on the scale.\n",
    "> * <B>Count data(or frequency data)</B>\n",
    "\n",
    "\n",
    "Examples of <B>ordinal</B> scales include placing in a race, or rating of degree of agreement.\n",
    "\n",
    "\n",
    "Examples of <B>nominal(or categorical) scales</B>, scales include political party affiliation, the face of a rolled die, and the result of a flipped coin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we care.\n",
    "We care about the scale type because the likelihood function must specify a probability distribution on the appropriate scale. \n",
    "\n",
    "If the scale has two nominal values, then a Bernoulli likelihood function may be appropriate. \n",
    "\n",
    "If the scale is metric, then a normal distribution may be appropriate as a likelihood function. \n",
    "\n",
    "★ Whenever we a choosing a model for data, we must answer the question, What kind of scale are we dealing with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.3 Linear function of a single metric predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Predicted variable(독립변수): y(metric)\n",
    "\n",
    "> Predictor(설명변수): x(metric)\n",
    "\n",
    "> Simply assumption: Linear relationship\n",
    "\n",
    ">> Linear functions preserve proportionality. If you double the input, then you double the output.\n",
    "Despite the fact that many real-world dependencies are non-linear, most are at least approximately linear over\n",
    "moderate ranges of the variables. \n",
    "\n",
    "\n",
    "The general mathematical form for a linear function of a single variable is\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "y = β_0 + β_1 x  ~~~~~~~~~~~~~~~~~~~~~~(14.1)              \n",
    "\\end{eqnarray*}\n",
    "\n",
    "The value of parameter $β_0 $ is called the y-intercept because it is the where the line intersects the y-axis when x = 0. \n",
    "\n",
    "The value of parameter $β_1 $ is called the slope because it indicates how much y increases when x increase by 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"1.png\"><figcaption> \n",
    "</figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In strict mathematical terminology, the type of transformation in Equation 14.1 is called <B><I>affine</I></B>. When $β_0 \\neq 0$, the transformation does not preserve proportionality.\n",
    "\n",
    "For example, consider y = 10 + 2x. When x is doubled from x = 1 to x = 2, y increases from y = 12 to y = 14, which is not doubling y. Nevertheless, the rate of increase in y is the same for all values of x: Whenever x increases by 1, y increases by 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 14.1.3.1 Re-parameterization to $x$ threshold form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation 14.1 can be algebraically re-arranged as follows:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "y = β_0 + β_1 x = \\beta_1(x-(-\\beta_0/\\beta_1))~~~~~~~~~~~~~~~~~~~~~~(14.2)              \n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    "This form of the equation is useful because it explicitly shows the value of the x-intercept, a.k.a. threshold, denoted $\\theta$.(The threshold is the value of x when y is zero.) This is sometimes also called the x intercept.\n",
    "\n",
    "\n",
    "The x-threshold form preserves proportionality for $x− \\theta$. \n",
    "\n",
    "> As an example, consider again the case of y = 10 + 2x. When changed to x-threshold form, it becomes y = 2(x + 5). When x changes from 1 to 2, x + 5 changes from 6 to 7, which is an increase of (7 − 6)/6 = 1/6. The resulting change in y is from 12 to 14, which is an increase of (14 − 12)/12 = 1/6. <B>Thus, a 1/6 increase in x − $\\theta$ results in a 1/6 increase in y.</B>\n",
    "\n",
    "\n",
    "<B>The threshold (i.e., x intercept) is often more meaningful than the y-intercept. </B>\n",
    "\n",
    "\n",
    "For example, suppose we are piloting a tugboat upstream on the Mississippi river, and we want to predict how much headway y we will gain against the current for a given setting of the throttle x. Suppose it is the case that y = −2 + 4x. This form of the equation indicates\n",
    "that when we apply zero engine power, that is when x = 0, then we lose 2 miles an hour, i.e., y = −2. \n",
    "\n",
    "\n",
    "<B>In other words, the y intercept tells us the baseline speed of the river current that we are trying to overcome. \n",
    "\n",
    "What may be more useful to know, however, is the amount of engine power we need to apply in order to overcome the current: How big must x be so that we are just matching the downstream pressure? </B>\n",
    "\n",
    "The answer to this question is the <B>threshold</B>, i.e., the value of x that makes y = 0. In our example, wherein y = −2 + 4x, the threshold is $\\theta$ = −(−2/4) = 0.5. In other words, when the throttle is set above the threshold\n",
    "of 0.5, then we make progress upstream because y > 0, but when the throttle is set below the threshold of 0.5, the we drift downstream because y < 0. \n",
    "\n",
    "<B>Thus, the more intuitive form of the “headway” equation is the x intercept form, y = 4(x− 0.5), because it shows explicitly that our headway is proportional to how much the throttle exceeds 0.5.</B>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of why we care. \n",
    "The likelihood function specifies the form of the dependency of y on x. When y and x are metric variables, the simplest form of dependency, both mathematically and intuitively, is one that preserves proportionality. The mathematical expression of this relation is a so-called linear function. The usual mathematical expression of a line is the y intercept form, but often a more intuitive expression is the x threshold form.\n",
    "\n",
    "<B>Linear functions form the core of most statistical models, so it is important to become facile with their algebraic forms and graphical representations.</B>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.4 Additive combination of metric predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want an increase in one predictor variable to predict the <B><I>same</I></B> proportional increase in the predicted variable <B><I>for any value of the other predictor variables,</I></B> then the predictions of the individual predictor variables must be added.\n",
    "\n",
    "\n",
    "In general, a linear combination of $K$ predictor variables has the form\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "y = β_0 + β_1 x_1 +\\cdots +β_K x_K  = \\beta_0 + \\sum{\\beta_k x_k}~~~~~~~~~~~~~~~~~~~~~~(14.3)              \n",
    "\\end{eqnarray*}\n",
    "\n",
    "Figure 14.2 shows examples of linear functions of <B>two</B> variables, $x_1$ and $x_2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"2.png\"><figcaption> \n",
    "</figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 14.1.4.1 Reparameterization to $x$ threshold form\n",
    "\n",
    ">For notational convenience, define the length of a vector $\\vec{β}$ = $<\\beta_1, ..., β_K >$ to be ||$\\vec{β}$||= ($\\sum_kβ_k)^{1/2} $. \n",
    "\n",
    "> With this new notation for length, Equation 14.3 can be algebraically re-expressed as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"3.png\"><figcaption> \n",
    "</figcaption></figure> $~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~(14.4)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice that when there is only a single predictor variable, i.e., when K = 1, then ||$\\vec{β}$|| = |$\\beta_1$|\n",
    "and Equation 14.4 reduces to Equation 14.2.\n",
    "\n",
    "\n",
    "> In Equation 14.4, the value of $\\theta$ is the (Euclidean) length of x when y = 0 and when x is in the direction of vector $<\\beta_1, ..., β_K >$. In other words, when $\\vec{x}$ = $\\theta$ ${\\vec{β}}\\over{||\\vec{β}||} $ \u001e",
    "then y = 0.\n",
    "\n",
    "\n",
    "> When the length of $\\vec{x}$ (in that direction) exceeds the threshold $\\theta$, then y > 0. The x threshold form in Equation 14.4 becomes especially useful when we consider <B>logistic regression</B> in future chapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Summary of section: \n",
    "When the influence of every individual predictor is unchanged\n",
    "by changing the values of other predictors, then the influences are additive. The combined\n",
    "influence of two or more predictors can be additive even if the individual influences are\n",
    "nonlinear. But if the individual influences are linear, and the combined influence is additive,\n",
    "then the overall combined influence is also linear. The formula of Equation 14.3, or its\n",
    "reparameterization in Equation 14.4, is known as the <B>linear model</B>. It forms the core of\n",
    "many statistical models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.5 Nonadditive interaction of metric predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combined influence of two predictors does $NOT$ have to be additive. \n",
    "\n",
    "\n",
    "Consider, for example, a person’s self-rating of happiness, predicted from his/her overall health and annualincome. It’s likely that if a person’s health is very poor, then the person is not happy, regardless of his/her income. And if the person has zero income, then the person is probably not happy, regardless of his/her health. But if the person is both healthy and rich, then the person is probably happy (despite celebrated counter-examples in the popular media)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"4.png\"><figcaption> \n",
    "</figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A graph of this sort of non-additive interaction between predictors appears in the upper left panel of Figure 14.3. \n",
    "\n",
    "<B>Notice that the graph of the interaction has a twist in it, but the graph ofthe additive combination is flat.And a non-additive interaction of predictors does not have to be multiplicative.</B>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.6 Nominal predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 14.1.6.1 Linear model for a single nominal predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The previous sections assumed that the predictor was metric. But what if the predictor is nominal, such as political party affiliation or gender? \n",
    "What is the simplest generic model for a metric variable predicted from a nominal variable? \n",
    "\n",
    "> Answer: The “natural” model has each value of x generate a particular deflection of y away from its baseline level. \n",
    "\n",
    "> For example, consider predicting height from sex (male or female). We can consider the overall average height across both sexes as the baseline height. When an individual has the value “male”, that adds an upward deflection to the predicted height. When an individual has the value “female”, that adds a downward deflection to the predicted height.\n",
    "\n",
    "\n",
    "> Expressing that idea in mathematical notation can get a little tricky. First consider the nominal predictor. We can’t represent it appropriately as a single scalar value, such as 1 through 5, because that would mean that level 1 is closer to level 2 than it is to level 5, which is not true of nominal values. Therefore, instead of representing the value of the nominal predictor by a single scalar value x, we will represent the nominal predictor by a vector $\\vec{x}$ = $<x_1, ..., x_j >$, where $J$ is the number of categories that the predictor has. When an individual has level j of the nominal predictor, this is represented by setting $x_j$ = 1 and $x_{i \\neq j}$ = 0. \n",
    "\n",
    "\n",
    "> Now that we have a formal representation for the nominal predictor variable, we can create a formal representation for the generic model of how the predictor influences the predicted variable. As mentioned above, the idea is that there is a baseline level of the predicted variable, and each category of the predictor indicates a deflection above or below that baseline level. We will denote the baseline value of the prediction as $β_0$. The deflection for the $j^{th}$ level of the predictor is denoted $β_j$. Then the predicted value is\n",
    "\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "y = β_0 + β_1 x_1 +\\cdots +β_K x_K  = \\beta_0 + \\vec{β}·\\vec{x}~~~~~~~~~~~~~~~~~~~~~~(14.5)              \n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    "> where the notation $\\vec{β}$·$\\vec{x}$ is sometimes called the “dot product” of the vectors.\n",
    "Notice that Equation 14.5 has a form very similar to the basic linear form of Equation 14.1. The conceptual analogy is this: In Equation 14.1 for a metric predictor, the slope $β_1$ indicates how much y changes when x changes from 0 to 1. In Equation 14.5 for a nominal predictor, the coefficient $β_j$ indicates how much y changes when x changes from neutral\n",
    "to category j.\n",
    "\n",
    "\n",
    "\n",
    "> The baseline is constrained so that the deflections sum to zero across the categories:\n",
    "\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "y =  \\sumβ_j = 0 ~~~~~~~~~~~~~~~~~~~~~~(14.6)              \n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    "> The expression of the model in Equation 14.5 is not complete without the constraint in 14.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"5.png\"><figcaption> \n",
    "</figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Figure 14.4 shows examples of a nominal predictor, expressed in terms of Equations 14.5 and 14.6). The left panel shows a case for which J = 2, and the right panel shows a case in which J = 5. Notice that the deflections from baseline sum to zero, as\n",
    "demanded by the constraint in Equation 14.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 14.1.6.2 Additive combination of nominal predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Suppose we have two (or more) nominal predictors of a metric value. \n",
    "\n",
    "> For example, we might be interested in predicting income as a function of political party affiliation and gender. Figure 14.4 showed examples of each of those predictors individually. What we do\n",
    "now is consider the joint influence of those predictors. If the two influences are merely\n",
    "additive, then the model from Equation 14.5 becomes\n",
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"13.png\"><figcaption> \n",
    "</figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"6.png\"><figcaption> \n",
    "</figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The left panel of Figure 14.5 shows an example of two nominal predictors that have additive effects on the predicted variable. In this case, the overall baseline is y = 6. When $x_1$ = < 1, 0 >, there is a deflection in y of −1, and when x1 = < 1, 0 >, there is a deflection in y of +1. This deflection by $x_1$  is the same at every level of $x_2$ . The deflections for the three levels of $x_2$  are +3, −2, and −1. These deflections are the same at all levels of $x_1$. Formally, the left panel of Figure 14.5 is expressed mathematically by the additive combination: \n",
    "\n",
    "> $y = 6 + <-1, 1>\\vec{x_1} + <3, -2, -1>\\vec{x_2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 14.1.6.3 Nonadditive interaction of nominal predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When the predictor variables are non-metric, it does not even make sense to talk about a\n",
    "multiplicative interaction, because there are no numerical values to multiply. For example,\n",
    "consider predicting annual income from political party affiliation and gender. Both predictors are nominal, so it makes no sense to “multiply” them. But it does make sense to\n",
    "consider non-additive combination of their influences.2\n",
    "For example, the overall influence of gender is that men, on average, have a higher\n",
    "income than women. The overall influence of political party affiliation is that Republicans,\n",
    "on average, have higher income than Democrats. But it may be that the influences combine\n",
    "non-additively: Perhaps people who are both Republican and male have a higher average\n",
    "income than would be predicted by merely adding the average income boosts for being\n",
    "Republican and for being male. (This interaction is not claimed to be true; it is being used\n",
    "only as a hypothetical example.)\n",
    "We need new notation to formalize the non-additive influence of a combination of nominal values. Just as −→x 1 refers to the value of predictor 1, and −→x 2 refers to the value of\n",
    "predictor 2, the notation −→x 1×2 will refer to a particular combination of values of predictors 1 and 2. If there are J1 levels of predictor 1 and J2 levels of predictor 2, then there are\n",
    "J1 × J2 combinations of the two predictors.\n",
    "A non-additive interaction of predictors is formally represented by including a term\n",
    "for the influence of combinations of predictors, beyond the additive influences, as follows:\n",
    "y = β0 +\n",
    "−→\n",
    "β 1−→x 1 + −→β 2−→x 2 + −→β 1×2−→x 1×2. Whenever the interaction coefficient −→β 1×2 is non-zero,\n",
    "the predicted value of y is not a mere addition of the separate influences of the predictors.\n",
    "The right panel of Figure 14.5 shows a graphical example of two nominal predictors\n",
    "that have interactive (i.e., non-additive) effects on the predicted variable. Notice, in the left\n",
    "pair of bars (x2 = h1, 0, 0i), that a change from x1 = h1, 0i to x1 = h0, 1i produces an\n",
    "increase of +2 in y, from y = 8 to y = 10. But for the middle pair of bars (x2 = h0, 1, 0i), a\n",
    "change from x1 = h1, 0i to x1 = h0, 1i produces an increase of −2 in y, from y = 5 to y = 3.\n",
    "Thus, the influence of x1 is not the same at all levels of x2.\n",
    "An interesting aspect of the pattern in the right panel of Figure 14.5 is that the average\n",
    "influences of x1 and x2 are the same as in the left panel. Overall, on average, going from\n",
    "x1 = h1, 0i to x1 = h0, 1i produces a change of+2 in y, in both the left and right panels. And\n",
    "overall, on average, for both panels it is the case that x2 = h1, 0, 0i is +3 above baseline,\n",
    "x2 = h0, 1, 0i is −2 below baseline, and x2 = h0, 0, 1i is −1 below baseline. The only\n",
    "difference between the two panels is that the combined influence of the two predictors\n",
    "equals the sum of the individual influences in the left panel, but the combined influence of\n",
    "the two predictors does not equal the sum of the individual influences in the right panel.\n",
    "An interaction between nominal predictors consists of a distinct deflection, for each\n",
    "specific combination of categorical values, away from the additive combination. The magnitude of the interactive deflection is whatever is left over after the additive effects have\n",
    "been applied to the baseline. The model that includes an interaction term can be written as\n",
    "\n",
    "\n",
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"14.png\"><figcaption> \n",
    "</figcaption></figure>\n",
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"15.png\"><figcaption> \n",
    "</figcaption></figure>\n",
    "\n",
    "\n",
    "> In these equations, the term −→x 1×2 has J1 times J2 components, all of which are zero except\n",
    "for a 1 at the particular combination of levels of x1 and x2. This mysterious and arcane\n",
    "notation will be revealed in all its majestic grandeur in Chapter 19. For now, the main\n",
    "point is to understand that the term “interaction” refers to a non-additive influence of the\n",
    "predictors on the predicted, regardless of whether the predictors are measured on a nominal\n",
    "scale or a metric scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.7 Linking combined predictors to the predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Once the predictor variables are combined, they need to be mapped to the predicted variable. This mathematical mapping is called the (inverse) link function, denoted by f() in the\n",
    "following equation:\n",
    "\n",
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"16.png\"><figcaption> \n",
    "</figcaption></figure>\n",
    "\n",
    "\n",
    "Until now, we have been assuming that the link function is merely the identity function,\n",
    "$f(x) = x$. For example, in Equation 14.9, y equals the linear combination of the predictors;\n",
    "there is no transformation of the linear combination before mapping the result to y.\n",
    "\n",
    "\n",
    "Before describing different link functions, it is important to make some clarifications\n",
    "of terminology and corresponding concepts. First, the function $f()$ in Equation 14.11 is\n",
    "usually called the inverse link function, because the link function itself is thought of as\n",
    "transforming the value y into a form that can be linked to the linear model. I will abuse\n",
    "convention and simply refer to either $f()$ or $f^{−1}()$ as <B>the link function</B>, and rely on context\n",
    "to disambiguate which direction of linkage is intended. The reason for this terminological\n",
    "sadism is that the arrows in hierarchical diagrams of Bayesian models will flow from the\n",
    "linear model toward the data, and therefore it is natural for the functions to map toward the\n",
    "data, as in Equation 14.11. But repeatedly referring to this function as the “inverse” link\n",
    "would strain my patience and violate my aesthetic sensibilities. Second, the value y that\n",
    "results from the link function $f(x)$ is not a data value per se. Instead, $f(x)$ is the value of\n",
    "a parameter that expresses some characteristic of the data, usually their mean. Therefore\n",
    "the function f() in Equation 14.11 is sometimes called the mean function, and is written\n",
    "$µ = f()$ instead of $y = f()$. I will not use this terminology because most students already\n",
    "think that “mean” means something else, namely the sum divided by N. The fact that y in\n",
    "Equation 14.11 is a parameter value and not a data value will become clear in subsequent\n",
    "sections as we encounter specific cases and examples.\n",
    "\n",
    "\n",
    "There are situations in which a non-identity link function is appropriate. Consider,\n",
    "for example, predicting response time as a function of amount of caffeine consumed. Response time declines as caffeine dosage increases, and therefore a linear prediction of RT from dosage would have a negative slope. This negative slope implies that for a very large\n",
    "dosage of caffeine, response time would become negative, which is impossible unless caffeine causes precognition (i.e., foreseeing events before they occur). Therefore a direct linear function cannot be used for extrapolation to large doses, and we might instead want\n",
    "to use an exponential link function such as $y = exp{(β_0 + β_1x)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"7.png\"><figcaption> \n",
    "</figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 14.1.7.1 The sigmoid (a.k.a. logistic) function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A frequently used link function is the sigmoid , also known as the logistic:\n",
    "\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "y = sig(x) = 1 /(1 + exp(−x))~~~~~~~~~~~~~~~~~~~~~~(14.12)              \n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    ">Notice the negative sign in front of the x. The sigmoid function ranges between 0 and 1.\n",
    "The sigmoid is nearly 0 when x is large negative, and is nearly 1 when x is large positive.\n",
    "For linear combinations of predictors, the sigmoid link function is most conveniently\n",
    "parameterized in x threshold form. For a single predictor variable, the sigmoid link function\n",
    "applied to the linear function of the predictor yields\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "y = sig (x; \\gamma, \\theta) = 1 /(1 + exp (−\\gamma (x − \\theta)))~~~~~~~~~~~~~~~~~~~~~~(14.13)              \n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    ">where $\\gamma$, called the gain, corresponds to $β_1$ in Equation 14.2, and where $\\theta$, called the threshold, corresponds to $−β_0/β_1$ in Equation 14.2.\n",
    "\n",
    "\n",
    ">Examples of Equation 14.13, i.e., the sigmoid of a single predictor, are shown in Figure 14.6. Notice that the threshold is the point on the x axis for which $y = 0.5.$ The gain indicates how steeply the sigmoid rises through that point.\n",
    "\n",
    "\n",
    ">Figure 14.7 shows examples of a sigmoid of two predictor variables. Above each panel\n",
    "is the equation for the corresponding graph. The equations are parameterized in x threshold\n",
    "form, as in Equation 14.4. In other words, $y = sig \u0010γ \u0010Pk wkxk − θ\u0011\u0011, with \u0010Pk w2k\u00111/2 = 1$.\n",
    "Notice, in particular, that the coefficients of $x_1$ and $x_2$ in the plotted equations do indeed have\n",
    "Euclidean length of 1.0. For example, in the upper-right panel, \u00100.71 2 + 0.71 2\u00111/2 = 1.0,\n",
    "except for rounding error.\n",
    "\n",
    "\n",
    ">The coefficients of the x variables determine the orientation of the sigmoidal “cliff”. For\n",
    "example, compare the two top panels in Figure 14.7, which differ only in the coefficients, not in gain or threshold. In the top left panel, the coefficients are $w_1 = 0$ and $w_2 = 1$, and\n",
    "the cliff rises in the $x_2$ direction. In the top right panel, the coefficients are $w_1 = 0.71$ and\n",
    "$w2 = 0.71$, and the cliff rises in the positive diagonal direction.\n",
    "\n",
    "\n",
    ">The threshold determines the position of the sigmoidal cliff. In other words, the threshold determines the x values for which y = 0.5. For example, compare the two left panels\n",
    "of Figure 14.7. The coefficients are the same, but the thresholds (and gains) are different.\n",
    "In the upper left panel, the threshold is zero, and therefore the mid-level of the cliff is over\n",
    "x2 = 0. In the lower left panel, the threshold is −3, and therefore the mid-level of the cliff\n",
    "is over x2 = −3.\n",
    "\n",
    "\n",
    ">The gain determines the steepness of the sigmoidal cliff. Again compare the two left\n",
    "panels of Figure 14.7. The gain of the upper left is 1, whereas the gain of the lower left is 2.\n",
    "\n",
    "\n",
    ">Terminology: The logit function. The inverse of the logistic function is called the\n",
    "logit function. For 0 < p < 1, logit(p) = log (p/(1 − p)). It is easy to show (try it!)\n",
    "that logit(sig(x)) = x, which is to say that the logit is indeed the inverse of the sigmoid.\n",
    "Some authors, and programmers, prefer to express the connection between predictors and\n",
    "predicted in the opposite direction, by first transforming the predicted variable to match the linear model. In other words, you may see the link expressed either of these ways:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "y = logistic (β_0 + β_1 x_1 + . . .) \n",
    "logit(y) = β0 + β1 x1 + . . .~~~~~~~~~~~~~~~~~~~~~~(14.13)              \n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    "The two expressions achieve the same result, mathematically. The difference between them\n",
    "is merely a matter of emphasis. In the first expression, the combination of predictors is\n",
    "transformed so it maps onto y expressed in its original scale. In the second expression, y is\n",
    "transformed onto a new scale, and that transformed value is modeled as a combination of\n",
    "predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"8.png\"><figcaption> \n",
    "</figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"9.png\"><figcaption> \n",
    "</figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 14.1.7.2 The cumulative normal (a.k.a. Phi) function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Another frequently used link function is the cumulative normal distribution. It is qualitatively very similar to the sigmoid or logistic function. Modelers will use the logistic or\n",
    "the cumulative normal depending on mathematical convenience or ease of interpretation.\n",
    "For example, when we consider ordinal predicted variables (in Chapter 21), it will be natural to model the responses in terms of a continuous underlying variable that has normally\n",
    "distributed variability, which leads to using the cumulative normal as a model of response\n",
    "probabilities.\n",
    "The cumulative normal is denoted Φ(x, µ, τ), where x is a real number and where µ\n",
    "and τ are parameter values, called the mean and precision of the normal distribution. The\n",
    "parameter µ governs the point at which the cumulative normal, Φ(x), equals 0.5. In other\n",
    "words, µ plays the same role as the threshold in the logistic sigmoid. The parameter τ\n",
    "governs the steepness of the cumulative normal function at x = µ. The τ parameter plays the same role as the gain parameter in the logistic sigmoid. A graph of a cumulative normal\n",
    "appears in Figure 14.8. For this example, µ = 0, and notice that Φ(0) = 0.5. This means\n",
    "that the area under the normal density to the left of 0 is 0.5.\n",
    "Terminology: The probit function. The inverse of the cumulative normal is called the\n",
    "probit function. (“Probit” stands for “probability unit”; Bliss, 1934). The probit function\n",
    "maps a value p, for 0.0 ≤ p ≤ 1.0, onto the infinite real line, and a graph of the probit\n",
    "function looks very much like the logit function. You may see the link expressed either of\n",
    "these ways: Traditionally, the transformation of y (in this case, the probit function) is called the link\n",
    "function, and the transformation of the linear combination of x (in this case, the Φ function)\n",
    "is called the inverse link function. As mentioned before, I abuse the traditional terminology\n",
    "and call either one a link function, relying on context to disambiguate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.8 Probabilistic prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the real world, there is always variation in y that we cannot predict from x. This unpredictable “noise” in y might be deterministically caused by sundry factors we have neither\n",
    "measured nor controlled, or the noise might be caused by inherent non-determinism in y. It\n",
    "does not matter either way because in practice the best we can do is predict the probability\n",
    "that y will have any particular value, dependent upon x. Therefore we use the deterministic\n",
    "value predicted by Equation 14.11 as the predicted tendency of y as a function of the predictors. We do not predict that y is exactly f\u0010β0 + β1 x1 + β2 x2 + β1×2 x1×2\u0011 because we would\n",
    "surely be wrong. Instead, we predict that y tends to be near f\u0010β0 + β1 x1 + β2 x2 + β1×2 x1×2\u0011.\n",
    "To make this notion of probabilistic tendency precise, we need to specify a probability\n",
    "distribution for y that depends on f\u0010β0 + β1 x1 + β2 x2 + β1×2 x1×2\u0011. To keep the notation\n",
    "tractable, first define µ = f\u0010β0 + β1 x1 + β2 x2 + β1×2 x1×2\u0011. Do not confuse this use of µ\n",
    "with the unrelated µ mentioned in the cumulative normal function. With this notation, we\n",
    "then denote the probability distribution of y as some to-be-specified probability density\n",
    "function, abbreviated as “pdf”:\n",
    "y ∼ pdf(µ [, τ, ...])\n",
    "The pdf might have various additional parameters, denoted by τ, ..., to specify its shape.\n",
    "Examples are provided in the next section, where all these ideas are brought together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.9 Formal expression of the GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the likelihood function specifies the probability of each possible predicted value\n",
    "y as a function of the predictor values x j and various parameter values β, τ etc. The generalized linear model can be written:\n",
    "(14,14)\n",
    "15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"10.png\"><figcaption> \n",
    "</figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"11.png\"><figcaption> \n",
    "</figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function f in Equation 14.14 is called the “link” function, because it links the combination of predictors to the predicted tendency. The optional parameters [, τ, ...] in Equation 14.15 may be needed for various types of the probability density function (pdf) that\n",
    "describe the probability distribution of y around the tendency µ.\n",
    "Figure 14.9 shows a random sample of points normally distributed around a line or\n",
    "plane. The upper panel illustrates a case of the generalized linear model of Equations 14.14\n",
    "and 14.15 in which there is a single predictor x, with β0 = 10 and β1 = 2. The link function\n",
    "is simply the identity function, f(β0 + β1 x) = β0 + β1 x. The probability density function is\n",
    "normal with a standard deviation of 2.0. Profiles of this normal density are superimposed\n",
    "on the graph to make it explicit. Notice that the normal density is always centered on the\n",
    "line that marks the predicted tendency as a function of the predictor.\n",
    "The lower panel of Figure 14.9 shows a case with two predictor variables. The predictors are combined linearly, with no interaction. The link function is the identity. The\n",
    "probability function is normal with a standard deviation of 4. Each randomly generated\n",
    "point is connected to the underlying linear core by a vertical dotted line, to explicitly indicate the random variation of the point from the plane. The plane marks the predicted\n",
    "tendency as a function of the predictors, and the data values are normally distributed above\n",
    "and below that tendency.\n",
    "Figure 14.10 shows another case of the GLM. In this case, the points are Bernoulli\n",
    "distributed around a sigmoid function of two predictors, as annotated at the top of the graph.\n",
    "There is a linear combination of predictors, with a sigmoid link function, and a Bernoulli\n",
    "probability function that defines the probability that y = 1. The graph shows that values of y\n",
    "can only be 0 or 1, and the sigmoid function defines the probability that y is 1 for particular predictor values. The sigmoidal surface plots the tendency that y = 1 as a function of the\n",
    "predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2 Cases of the GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 14.1, p. 312, displays the various cases of the generalized linear model that are considered in this book. Subsequent chapters of the book progress through the table in reading\n",
    "order: left to right within rows, then top to bottom across rows.\n",
    "The first row of Table 14.1 lists cases for which the predicted variable is metric. Moving\n",
    "from left to right within this row, the first column indicates a situation in which there is only\n",
    "a single group, and the predicted value for the group is simply the mean of the group. In\n",
    "this situation, there is no need to explicitly denote a predictor variable, and instead the mean\n",
    "of the group can be denoted by a single parameter, β0. This situation corresponds to what\n",
    "classical null hypothesis significance testing (NHST) calls a single-group t-test. This case\n",
    "is described in its Bayesian setting in Chapter 15.\n",
    "Moving to the next column, there is a single metric predictor. This corresponds to socalled “simple linear regression”, and is explored in Chapter 16. By inspecting the equation\n",
    "for the GLM in the cell, you can see that the only difference from the previous cell is the\n",
    "inclusion of the predictor x1 and its coefficient β1 .\n",
    "Moving rightward to the next column, we come to the scenario involving two or more\n",
    "metric predictors, which corresponds to “multiple regression”, and is explored in Chapter 17. By examining the equations for the GLM in the cell, you can see that the basic form\n",
    "is the same, but merely with extra terms added for the additional predictors.\n",
    "The next two columns involve nominal predictors, instead of metric predictors, with the\n",
    "penultimate column devoted to a single predictor and the final column devoted to two or\n",
    "more predictors. The last two columns correspond to what NHST calls “oneway ANOVA”\n",
    "and “multifactor ANOVA”. If that terminology is unfamiliar to you, don’t worry, it will be\n",
    "explained in Chapters 18 and 19.\n",
    "In all the cases in the first row, the link function is the identity, and the probability distribution for the metric predicted values in assumed to be normal. When we move to the\n",
    "second row, however, the predicted variable is dichotomous, and therefore the probability\n",
    "distribution for y is a Bernoulli distribution. The link function, which connects the predictors to the probability that y = 1, is assumed to be the sigmoid, i.e., logistic function. When\n",
    "the predictors are metric, this situation is generically referred to as “logistic regression” and\n",
    "is discussed in Chapter 20. The case of nominal predictors is also discussed.\n",
    "Finally, the bottom row of Table 14.1 lists cases for which the predicted variable is\n",
    "ordinal. These cases are considered in Chapter 21. Notice that the link function is the\n",
    "cumulative normal instead of the sigmoid, and the ordinal values are generated by multiplecategory generalization of the Bernoulli function, denoted by dcat. Again, this will be\n",
    "explained at length in the forthcoming chapters. The point here is for you to see the overall\n",
    "organization of topics, and to see how all these cases are variations of the same underlying\n",
    "structure.\n",
    "The table can be expanded with additional rows and columns, but then it gets too big\n",
    "to display easily. Additional columns would include combinations of metric and nominal\n",
    "predictors. But it turns out that it is easy in Bayesian models to combine metric and nominal predictors, once you know how to handle metric and nominal predictors individually.\n",
    "Additional rows would involve different types of predicted variables. In particular, a fourth row would include count data for the predicted values. We will, in fact, cover one such\n",
    "case, as described in the next section. When the predicted data are count values, a natural\n",
    "link function is the exponential, and a natural pdf is the Poisson distribution, which will\n",
    "be defined later in the book (Section 22.1.3). In summary, the rows of the table refer to\n",
    "differently scaled predicted values, with their corresponding link functions and pdf’s:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"12.png\"><figcaption> \n",
    "</figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2.1 Two or more nominal variables predicting frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will also consider the situation in which there are two or more nominal variables\n",
    "used as predictors of a frequency count. A frequency count, i.e., how many times something\n",
    "happened, is a special case of a metric scale, but because its values fall at discrete levels,\n",
    "namely non-negative integers, this situation will have a different sort of likelihood distribution. This type of situation, with nominal predictors and frequency-count predicted values,\n",
    "is often called “contingency table analysis” and a typical NHST analysis conducts a “chisquare test of independence of attributes”. We explore Bayesian analysis of this situation in\n",
    "Chapter 22.\n",
    "Here is a brief summary of how contingency tables are analyzed using a model much\n",
    "like those in Table 14.1. In fact, a fourth row could be added to Table 14.1, with the predicted type labeled frequency count, and the model falling in the final column, under two\n",
    "nominal predictors. As a concrete example, suppose we measure political affiliation and\n",
    "religious affiliation of a set of people, and for a sample of people we count how many\n",
    "occurrences there are of each combination. We are interested in analyzing possible relationships between political and religious affiliations. Suppose we conduct a poll for one\n",
    "week. We happen to record 27 people who are Democrats and Unitarians. This observed\n",
    "frequency reflects an underlying rate at which that combination is generated by this sort\n",
    "of poll, i.e., the underlying rate for Unitarian Democrats is roughly 27 people per week.\n",
    "The observed rate (i.e., frequency per unit time) for each combination of nominal values\n",
    "is thought to reflect the true underlying rate at which that combination is generated by the\n",
    "world. We conceive of the observed rate as being a random sample from a true underlying\n",
    "rate denoted by λ. The probability of any particular observed rate, given an underlying\n",
    "rate of λ, is modeled by a Poisson distribution, which is denoted as freq ∼ dpois(λ). The\n",
    "Poisson distribution was smuggled into the text back in Exercise 11.3, p. 235, which I’m\n",
    "sure is still as fresh in your memory as a beached fish. Don’t worry, the Poisson will be\n",
    "explained again later (Section 22.1.3). The Poisson distribution specifies a probability for\n",
    "each possible observed rate. The Poisson puts highest probabilities on rates near λ.\n",
    "Our goal is to estimate the underlying rates at which each nominal combination is produced. But more than that, we would like to know if the attributes occur independently of\n",
    "each other, or instead covary in some way. For example, if political and religious affiliation\n",
    "are independent, then there should be the same proportion of Unitarians among Democrats\n",
    "as among Republicans. Mathematically, independence means p(Unitarian&Democrat) =\n",
    "p(Unitarian) × p(Democrat) and p(Unitarian&Republican) = p(Unitarian) × p(Republican)\n",
    "and so on for every combination of attribute values. To shorten the expressions, I’ll substi tute U for Unitarian and D for Democrat, whereby independence means\n",
    "p(U&D) = p(U) × p(D)\n",
    "and so on for every combination of attributes. That expression for probabilities corresponds\n",
    "to the following expression in terms of frequencies:\n",
    "freq(U&D)/N = freq(U)/N × freq(D)/N\n",
    "which can be re-arranged as\n",
    "freq(U&D) = freq(U) × freq(D) × 1/N.\n",
    "Notice that independence is expressed as a multiplicative product of attribute influences.\n",
    "But all the models we’ve considered in this chapter used an additive sum of predictor influences. To be able to use our familiar additive models, we’ll transform the frequencies by a\n",
    "logarithm, because the logarithm of the product of values equals the sum of the logarithms\n",
    "of the values. In other words,\n",
    "log(freq(U&D)) = log(freq(U)) + log(freq(D)) + log(1/N).\n",
    "The notation “log(freq(value))” gets cumbersome, so we’ll substitute the notation βv. Thus,\n",
    "log(freq(U&D)) = βU + βD + β0,\n",
    "where β0 stands in for the constant log(1/N). Finally, it’s unintuitive to talk about the\n",
    "logarithms of frequencies, so we’ll exponentiate to get rid of the leading logarithm, yielding:\n",
    "freq(U&D) = exp (βU + βD + β0) .\n",
    "To summarize, if independence is true, then the expression above should be true, for every\n",
    "combination of attribute values.\n",
    "But of course the attributes are usually not independent, and we would like some measure of lack of independence. We already have such a measure in the context of linear\n",
    "models, namely, the interaction term. Thus, we will include an interaction term that estimates deviation from independence. Thus, our model ends up being as follows. For two\n",
    "nominal attributes, we put the observed frequencies in a table, with one attribute’s values\n",
    "listed down the rows, and the other attribute’s values listed across the columns. The frequency in the rth row and cth column is denoted freqrc, and the underlying rate for that cell\n",
    "is denoted λ\n",
    "rc. The model looks like this:\n",
    "λ\n",
    "rc = exp (β0 + βr + βc + βr ×c)\n",
    "freqrc ∼ dpois(λrc) (14.16)\n",
    "with the usual constraints (from Equation 14.10)\n",
    "Xr\n",
    "βr = 0 and X\n",
    "c\n",
    "βc = 0 and X\n",
    "r\n",
    "βr ×c,r,c = 0 ∀c and X\n",
    "c\n",
    "βr×c,r,c = 0 ∀r\n",
    "The point of this over-fast prelude to contingency table analysis is merely to demonstrate that the core of the model we’ll be using is the same as the linear model that was\n",
    "mentioned for multifactor ANOVA in Table 14.1. Thus, all the applied analyses we’ll see\n",
    "in the remainder of the book are based on the GLM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
