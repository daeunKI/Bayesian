{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Chapter 14.  Overview of the GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## -----------------------------------------------------------------------------------------------------------------------------\n",
    "## Contents\n",
    "### 14.1 The GLM\n",
    "### 14.2 Cases of the GLM\n",
    "## -----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1 The Generalized Linear Model(GLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.1 Predictor and predicted variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to predict someone’s weight from their height. \n",
    "> Predicted variable(독립변수): weight\n",
    "\n",
    "> Predictor(설명변수): height\n",
    "\n",
    "suppose we want to predict high school grade point average (GPA) from Scholastic Aptitude Test (SAT) score and family income.\n",
    "> Predicted variable(독립변수): GPA\n",
    "\n",
    "> Predictor(설명변수): SAT and income\n",
    "\n",
    "** The value of the predictor variable comes from\n",
    "“outside” the system being modeled, whereas the value of the predicted variable depends\n",
    "on the value of the predictor variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####The key mathematical difference between predictor and predicted variables is that the likelihood function.\n",
    "(Likelihood function: expresses the probability of values ofthe predicted variable as a function of values of the predictor variable. The likelihood function does not describe the probabilities of values of the predictor variable.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In experimental settings, the variables that are actually manipulated and set by the\n",
    "experimenter are the independent variables. In this context of experimental manipulation, the values of the independent variables truly are (in principle, at least) independent of the values of other variables, because the experimenter has intervened to arbitrarily set the values of the independent variables. But sometimes a non-manipulated variable is also referred to as “independent”, merely as a way to indicate that it is being used as a predictor variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among non-manipulated variables, the roles of predicted and predictor are arbitrary, determined only by the interpretation of the analysis. \n",
    "\n",
    "> Consider, for example, people’s weights and heights. We could be interested in predicting a person’s weight from his/her height, or we could be interested in predicting a person’s height from his/her weight.\n",
    "\n",
    "Prediction is merely a mathematical dependency, not necessarily a description of underlying causal relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as “prediction” does not imply causation, “prediction” also does not imply any temporal relation between the variables. \n",
    "\n",
    "> For example, we may want to predict a person’s\n",
    "sex, male or female, from his/her height. Because males tend to be taller than females, this\n",
    "prediction can be made with better than chance accuracy. But a person’s sex is not caused\n",
    "by his/her height, nor does a person’s sex occur only after their height is measured. \n",
    "\n",
    "Thus,\n",
    "we can “predict” a person’s sex from his/her height, but this does not mean that the person’s\n",
    "sex occurred later in time than his/her height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In summary: \n",
    "#### All manipulated independent variables are predictor variables, not predicted. \n",
    "####Some dependent variables can take on the role of predictor variables, if desired. All predicted variables are dependent variables. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we care.\n",
    "We care about these distinctions between predicted and predictor variables <B>because the likelihood function is a mathematical description of the dependency of the predicted variable on the predictor variable.</B>\n",
    "\n",
    "The first thing we have to do in statistical inference is identify what variables we are interested in predicting, on the basis of what\n",
    "predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.2 Scale types: metric, ordinal, nominal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Items can be measured on different scales. For example, the participants in a foot race can\n",
    "be measured either by the time they took to run the race, or by their placing in the race (1st,\n",
    "2nd, 3rd, etc.), or by the name of the team they represent. These three measurements are\n",
    "examples of metric, ordinal, and nominal scales, respectively (Stevens, 1946).\n",
    "Examples ofmetric scales include response time (i.e., latency or duration), temperature,\n",
    "height, and weight. Those are actually cases of a specific type of metric scale, called a ratio\n",
    "scale, because they have a natural zero point on the scale. The zero point on the scale\n",
    "corresponds to there being a complete absence of the stuff being measured. For example,\n",
    "when the duration is zero, there has been no time elapsed, and when the weight is zero,\n",
    "there is no downward force. Because these scales have a natural zero point, it is meaningful\n",
    "to talk about ratios of amounts being measured, and that is why they are called ratio scales.\n",
    "For example, it is meaningful to say that taking 2 minutes to solve a problem is twice as\n",
    "long as taking 1 minute to solve the problem. On the other hand, the scale of historical\n",
    "time has no known absolute zero. We cannot say, for example, that there is twice as much\n",
    "time in January 2nd as there is time in January 1st. We can refer to the duration since some\n",
    "arbitrary reference point, but we cannot talk about the absolute amount of time in any given\n",
    "moment. Scales that have no natural zero are called interval scales because all we know\n",
    "about them is the amount of stuff in an interval on the scale, not the amount of stuff at a\n",
    "point on the scale. Despite the conceptual difference between ratio and interval scales, we\n",
    "will lump them together into the category of metric scales.\n",
    "A special case of metric-scaled data is count data, also called frequency data. For example, the number of cars that pass through an intersection during an hour is a count. The\n",
    "number of poll respondents who say they belong to a particular political party is a count.\n",
    "Count data can only have values that are non-negative integers. Distances between counts\n",
    "have meaning, and therefore the data are metric, but because the data cannot be negative and are not continuous, they are treated with different mathematical forms than continuous,\n",
    "real-valued metric data.\n",
    "Examples of ordinal scales include placing in a race, or rating of degree of agreement.\n",
    "When we are told that, in a race, Jane came in first, Jill came in second, and Jasmine came\n",
    "in third, we only know the order. We do not know whether Jane beat Jill by a nose or by a\n",
    "mile. There is no distance or metric information in an ordinal scale. As another example,\n",
    "many polls have ordinal response scales: Indicate how much you agree with this statement:\n",
    "“Bayesian statistical inference is better than null hypothesis significance testing”, with 5 =\n",
    "strongly agree, 4 = mildly agree, 3 = neither agree nor disagree, 2 = mildly disagree, and\n",
    "1 = strongly disagree. Notice that there is no metric information in the response scale,\n",
    "because we cannot say the difference between ratings of 5 and 4 is the same amount of\n",
    "difference as between ratings of 4 and 3.\n",
    "Examples of nominal, a.k.a. categorical, scales include political party affiliation, the\n",
    "face of a rolled die, and the result of a flipped coin. For nominal scales, there is neither\n",
    "distance between categories nor order between categories. For example, suppose we measure the political party affiliation of a person. The categories of the scale might be Green,\n",
    "Democrat, Republican, Libertarian, and Other. While some political theories might infer\n",
    "that the parties fall on some underlying liberal-conservative scale, there is no such scale\n",
    "in the actual categorical values themselves. In the actual categorical labels there is neither\n",
    "distance nor ordering.\n",
    "In summary, if two items have different nominal values, all we know is that the two\n",
    "items are different (and what categories they are in). On the other hand, if two items have\n",
    "different ordinal values, we know that the two items are different and we know which one is\n",
    "“larger” than the other, but not how much larger. If two items have different metric values,\n",
    "then we know that they are different, which one is larger, and how much larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we care.\n",
    "We care about the scale type because the likelihood function must specify a probability distribution on the appropriate scale. If the scale has two nominal values,\n",
    "then a Bernoulli likelihood function may be appropriate. If the scale is metric, then a normal distribution may be appropriate as a likelihood function. Whenever we a choosing a\n",
    "model for data, we must answer the question, What kind of scale are we dealing with?\n",
    "In the following sections, we will first consider the case of a metric predicted variable\n",
    "with metric predictors. In that context of all metric variables, we will develop the concepts of linear functions and interactions. Once those concepts are established for metric\n",
    "predictors, the notions will be extended to nominal predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.3 Linear function of a single metric predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have identified one variable to be predicted, which we’ll call y, and one variable\n",
    "to be the predictor, which we’ll call x. Suppose we have determined that both variables are\n",
    "metric. The next issue we need to address is how to model a relationship between x and y.\n",
    "There are many possible dependencies of y on x, and the particular form of the dependency\n",
    "is determined by the specific meanings and nature of the variables. But in general, across all\n",
    "possible domains, what is the most basic or simplistic dependency of y on x that we might\n",
    "consider? The usual answer to this question is, a linear relationship. A linear function is the\n",
    "generic, “vanilla”, off-the-shelf dependency that is used in statistical models. The methods\n",
    "can be generalized to other models when needed.\n",
    "Linear functions preserve proportionality. If you double the input, then you double the\n",
    "output. If cost of a book is a linear function of the number of pages, then when the number of pages is reduced 10%, the cost should be reduced 10%. If automobile speed is a linear\n",
    "function of gas delivery to the engine, then when you press the pedal 20% further, the car\n",
    "should go 20% faster. Non-linear functions do not preserve proportionality. For example,\n",
    "in actuality, car speed is not a linear function of gas delivery. At higher and higher speeds,\n",
    "it takes proportionally more and more gas to make the car go faster. Despite the fact that\n",
    "many real-world dependencies are non-linear, most are at least approximately linear over\n",
    "moderate ranges of the variables. For example, if you have twice the wall area, it takes\n",
    "approximately twice the amount of paint. It is also the case that linear relationships are\n",
    "intuitively prominent (Brehmer, 1974; Hoffman, Earle, & Slovic, 1981; Kalish, Griffiths, &\n",
    "Lewandowsky, 2007). Linear relationships are the easiest to think about: Turn the steering\n",
    "wheel twice as far, and the car should turn twice as sharp. Turn the volume knob 50%\n",
    "higher, the loudness should increase 50%.\n",
    "The general mathematical form for a linear function of a single variable is\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "y = β_0 + β_1 x\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    "When values of x and y that satisfy Equation 14.1 are plotted, they form a line. Examples\n",
    "are shown in Figure 14.1. The value of parameter β0 is called the y-intercept because it is\n",
    "the where the line intersects the y-axis when x = 0. The left panel of Figure 14.1 shows two\n",
    "lines with different y-intercepts. The value of parameter β1 is called the slope because it\n",
    "indicates how much y increases when x increase by 1. The right panel of Figure 14.1 shows\n",
    "two lines with the same intercept but different slopes.\n",
    "In strict mathematical terminology, the type of transformation in Equation 14.1 is called\n",
    "affine. When β0 , 0, the transformation does not preserve proportionality. For example,\n",
    "consider y = 10 + 2x. When x is doubled from x = 1 to x = 2, y increases from y = 12 to\n",
    "y = 14, which is not doubling y. Nevertheless, the rate of increase in y is the same for all\n",
    "values of x: Whenever x increases by 1, y increases by 2. Equation 14.1 can be algebraically\n",
    "re-arranged so that it does preserve proportionality, as will be shown next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"1.png\"><figcaption> \n",
    "</figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 14.1.3.1 Reparameterization to x threshold form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation 14.1 can be algebraically re-arranged as follows:\n",
    "(14.2)\n",
    "This form of the equation is useful because it explicitly shows the value of the x-intercept,\n",
    "a.k.a. threshold, denoted θ. (Do not confuse this use of the symbol θ with different uses in\n",
    "previous chapters.) The threshold is the value of x when y is zero. This is sometimes also\n",
    "called the x intercept.\n",
    "The x-threshold form preserves proportionality for x− θ. As an example, consider again\n",
    "the case of y = 10 + 2x. When changed to x-threshold form, it becomes y = 2(x + 5). When\n",
    "x changes from 1 to 2, x + 5 changes from 6 to 7, which is an increase of (7 − 6)/6 = 1/6.\n",
    "The resulting change in y is from 12 to 14, which is an increase of (14 − 12)/12 = 1/6.\n",
    "Thus, a 1/6 increase in x − θ results in a 1/6 increase in y.\n",
    "The threshold (i.e., x intercept) is often more meaningful than the y-intercept. For\n",
    "example, suppose we are piloting a tugboat upstream on the Mississippi river, and we want\n",
    "to predict how much headway y we will gain against the current for a given setting of the\n",
    "throttle x. Suppose it is the case that y = −2 + 4x. This form of the equation indicates\n",
    "that when we apply zero engine power, that is when x = 0, then we lose 2 miles an hour,\n",
    "i.e., y = −2. In other words, the y intercept tells us the baseline speed of the river current\n",
    "that we are trying to overcome. What may be more useful to know, however, is the amount\n",
    "of engine power we need to apply in order to overcome the current: How big must x be\n",
    "so that we are just matching the downstream pressure? The answer to this question is the\n",
    "threshold, i.e., the value of x that makes y = 0. In our example, wherein y = −2 + 4x, the\n",
    "threshold is θ = −(−2/4) = 0.5. In other words, when the throttle is set above the threshold\n",
    "of 0.5, then we make progress upstream because y > 0, but when the throttle is set below\n",
    "the threshold of 0.5, the we drift downstream because y < 0. Thus, the more intuitive form\n",
    "of the “headway” equation is the x intercept form, y = 4(x− 0.5), because it shows explicitly\n",
    "that our headway is proportional to how much the throttle exceeds 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of why we care. \n",
    "The likelihood function specifies the form of the dependency of y on x. When y and x are metric variables, the simplest form of dependency, both\n",
    "mathematically and intuitively, is one that preserves proportionality. The mathematical expression of this relation is a so-called linear function. The usual mathematical expression of\n",
    "a line is the y intercept form, but often a more intuitive expression is the x threshold form.\n",
    "Linear functions form the core of most statistical models, so it is important to become facile\n",
    "with their algebraic forms and graphical representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.4 Additive combination of metric predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have more than one predictor variable, what function should we use to combine the\n",
    "influences of all the predictor variables? If we want the combination to be linear in each of the predictor variables, then there is just one answer: Addition. In other words, if we\n",
    "want an increase in one predictor variable to predict the same proportional increase in the\n",
    "predicted variable for any value of the other predictor variables, then the predictions of the\n",
    "individual predictor variables must be added.\n",
    "In general, a linear combination of K predictor variables has the form\n",
    "(14,3)\n",
    "Figure 14.2 shows examples of linear functions of two variables, x1 and x2. The graphs\n",
    "show y plotted only over a the domain with 0 ≤ x1 ≤ 10 and 0 ≤ x2 ≤ 10. It is important to\n",
    "realize that the plane extends from minus to plus infinity, and the graphs only show a small\n",
    "region. Notice in the upper left panel, where y = 0 + 1 x1 + 0x2, that when x1 = 10, then\n",
    "y = 10, regardless of the value of x2. The plane tilts upward in the x1 direction, but the plane\n",
    "is horizontal in the x\n",
    "2 direction. The opposite is true in the upper-right panel: The plane\n",
    "tilts upward in the x2 direction, but the plane is horizontal in the x1 direction, because there\n",
    "y = 0 + 0x1 + 2x2. The lower-left panel shows the two influences added: y = 0 + 1 x1 + 2x2.\n",
    "Notice that the slope in the x2 direction is steeper than in the x1 direction. Most importantly,\n",
    "notice that the slope in the x2 direction is the same at any specific value of x1 . For example,\n",
    "when x\n",
    "1 = 0, y rises from y = 0 to y = 20, i.e. an increase of 20, when x2 goes from x2 = 0\n",
    "to x\n",
    "2 = 10. And when x1 = 10, y rises from y = 10 to y = 30, again an increase of 20, when\n",
    "x\n",
    "2 goes from x2 = 0 to x2 = 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"2.png\"><figcaption> \n",
    "</figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 14.1.4.1 Reparameterization to x threshold form\n",
    "\n",
    ">For notational convenience, define the length of a vector −→β = hβ1, ..., βK i to be \n",
    "−→β\n",
    "\u0010Pk β2k\u00111/2. This may look complicated, but it’s merely the everyday formula for distance\n",
    "from the Pythagorean theorem. Carpenters all memorize a handy special case of this relationship, known as the “3-4-5 rule”: When the lengths of the short edges of a right-angled\n",
    "triangle are 3 and 4, then the length of the long edge is exactly 5, because 5 = (32 + 42)1/2.\n",
    "(Carpenters actually use the 3-4-5 rule to infer an angle from lengths, rather than infer a\n",
    "third length from two lengths and an angle. Carpenters know that if a triangle has edge\n",
    "lengths of 3, 4, and 5, then the angle between the short edges is exactly 90 degrees.) With\n",
    "this new notation for length, Equation 14.3 can be algebraically re-expressed as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"3.png\"><figcaption> \n",
    "</figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice that when there is only a single predictor variable, i.e., when K = 1, then\n",
    "−→β\n",
    " = |β1|\n",
    "and Equation 14.4 reduces to Equation 14.2.\n",
    "In Equation 14.4, the value of θ is the (Euclidean) length of x when y = 0 and when x\n",
    "is in the direction of vector hβ1, ..., βKi. In other words, when −→x = θ −→β \u001e",
    "\n",
    "−→β\n",
    " , then y = 0.\n",
    "When the length of−→x (in that direction) exceeds the threshold θ, then y > 0. The x threshold\n",
    "form in Equation 14.4 becomes especially useful when we consider logistic regression in\n",
    "future chapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Summary of section: \n",
    "When the influence of every individual predictor is unchanged\n",
    "by changing the values of other predictors, then the influences are additive. The combined\n",
    "influence of two or more predictors can be additive even if the individual influences are\n",
    "nonlinear. But if the individual influences are linear, and the combined influence is additive,\n",
    "then the overall combined influence is also linear. The formula of Equation 14.3, or its\n",
    "reparameterization in Equation 14.4, is known as the linear model. It forms the core of\n",
    "many statistical models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.5 Nonadditive interaction of metric predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combined influence of two predictors does not have to be additive. Consider, for example, a person’s self-rating of happiness, predicted from his/her overall health and annualincome. It’s likely that if a person’s health is very poor, then the person is not happy, regardless of his/her income. And if the person has zero income, then the person is probably\n",
    "not happy, regardless of his/her health. But if the person is both healthy and rich, then the\n",
    "person is probably happy (despite celebrated counter-examples in the popular media).\n",
    "A graph of this sort of non-additive interaction between predictors appears in the upper\n",
    "left panel of Figure 14.3. The vertical axis, labeled y, is happiness. The horizontal axes,\n",
    "x\n",
    "1 and x2, are health and income. Notice that if either x1 = 0 or x2 = 0, then y = 0.\n",
    "But if both x\n",
    "1 > 0 and x2 > 0, then y > 0. The specific form of interaction plotted here\n",
    "is multiplicative: y = 0 + 0x1 + 0x2 + .2x1 x2. For comparison, the upper-right panel of\n",
    "Figure 14.3 shows a non-interactive (i.e., additive) combination of x1 and x2. Notice that\n",
    "the graph of the interaction has a twist in it, but the graph ofthe additive combination is flat.\n",
    "The lower-left panel of Figure 14.3 shows a multiplicative interaction in which the individual predictors increase the outcome, but the combined variables decrease the outcome.\n",
    "A real-world example of this occurs with some drugs: Individually, each of two drugs might\n",
    "improve symptoms, but when taken together, the two drugs might interact and cause a decline in health. As another example, consider lighter-than-air travel, i.e., ballooning. The\n",
    "levity of a balloon is increased by fire, as in hot air balloons. And the levity of a balloon is\n",
    "increased by hydrogen, as in many early-20th century blimps and dirigibles. But the levity\n",
    "of a balloon is dramatically decreased by the combination of fire and hydrogen.\n",
    "The lower-right panel of Figure 14.3 shows a multiplicative interaction in which the\n",
    "direction of influence of one variable depends on the magnitude of the other variable. Notice\n",
    "that when x\n",
    "2 = 0, then an increase in the x1 variable leads to a decline in y. But when\n",
    "x\n",
    "2 = 10, then an increase in the x1 variable leads to an increase in y. Again, the graph of\n",
    "the interaction shows a twist and is not flat.\n",
    "A non-additive interaction of predictors does not have to be multiplicative. Other types\n",
    "of interaction are possible. The type of interaction is motivated by idiosyncratic theories\n",
    "in different variables in different application domains. Consider, for example, predicting\n",
    "the magnitude of gravitational force between two objects, from three predictor variables:\n",
    "mass of object one, mass of object two, and the distance between the objects. The force\n",
    "is proportional to the product (i.e., multiplication) of their two masses. But the force is\n",
    "proportional to the masses divided by the squared distance between them.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"4.png\"><figcaption> \n",
    "</figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.6 Nominal predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 14.1.6.1 Linear model for a single nominal predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The previous sections assumed that the predictor was metric. But what if the predictor is\n",
    "nominal, such as political party affiliation or gender? What is the simplest generic model\n",
    "for a metric variable predicted from a nominal variable? Answer: The “natural” model\n",
    "has each value of x generate a particular deflection of y away from its baseline level. For\n",
    "example, consider predicting height from sex (male or female). We can consider the overall\n",
    "average height across both sexes as the baseline height. When an individual has the value\n",
    "“male”, that adds an upward deflection to the predicted height. When an individual has the\n",
    "value “female”, that adds a downward deflection to the predicted height.\n",
    "Expressing that idea in mathematical notation can get a little tricky. First consider the\n",
    "nominal predictor. We can’t represent it appropriately as a single scalar value, such as 1\n",
    "through 5, because that would mean that level 1 is closer to level 2 than it is to level 5,\n",
    "which is not true of nominal values. Therefore, instead of representing the value of the\n",
    "nominal predictor by a single scalar value x, we will represent the nominal predictor by a\n",
    "vector −→x = hx1, . . ., xJ i, where J is the number of categories that the predictor has. When\n",
    "an individual has level j of the nominal predictor, this is represented by setting x\n",
    "j = 1\n",
    "and x\n",
    "i, j = 0. For example, suppose x is sex, with level 1 being “male” and level 2 being\n",
    "female (so J = 2). Then “male” is represented as −→x = h1, 0i and “female” is represented\n",
    "as −→x = h0, 1i. As another example, suppose that the predictor is political party affiliation,\n",
    "with Green as level 1, Democrat as level 2, Republican as level 3, Libertarian as level 4,\n",
    "and Other as level 5. Then Democrat is represented as −→x = h0, 1, 0, 0, 0i, and Libertarian\n",
    "is represented as −→x = h0, 0, 0, 1, 0i. Political party affiliation is being treated here as a\n",
    "categorical label only, with no ordering along a liberal-conservative scale.\n",
    "Now that we have a formal representation for the nominal predictor variable, we can\n",
    "create a formal representation for the generic model of how the predictor influences the\n",
    "predicted variable. As mentioned above, the idea is that there is a baseline level of the\n",
    "predicted variable, and each category of the predictor indicates a deflection above or below\n",
    "that baseline level. We will denote the baseline value of the prediction as β0. The deflection\n",
    "for the jth level of the predictor is denoted βj. Then the predicted value is\n",
    "(14.5)\n",
    "where the notation −→β · −→x is sometimes called the “dot product” of the vectors.\n",
    "Notice that Equation 14.5 has a form very similar to the basic linear form of Equation 14.1. The conceptual analogy is this: In Equation 14.1 for a metric predictor, the slope\n",
    "β1 indicates how much y changes when x changes from 0 to 1. In Equation 14.5 for a nominal predictor, the coefficient β\n",
    "j indicates how much y changes when x changes from neutral\n",
    "to category j.\n",
    "There is one more consideration when expressing the influence of a nominal predictor as\n",
    "in Equation 14.5: How should the baseline value be set? Consider, for example, predicting\n",
    "height from sex. We could set the baseline height to be zero. Then the deflection from\n",
    "baseline for male might be 5’10” (say), and the deflection from baseline for female might\n",
    "be 5’4” (say). On the other hand, we could set the baseline height to be 5’7”. Then the\n",
    "deflection from baseline for male would be +3”, and the deflection from baseline for female\n",
    "would be −3”. The second way of setting the baseline is the typical way it is done in generic\n",
    "statistical modeling. In other words, the baseline is constrained so that the deflections sum\n",
    "to zero across the categories:\n",
    "(14.6)\n",
    "The expression of the model in Equation 14.5 is not complete without the constraint in\n",
    "14.6.\n",
    "Figure 14.4 shows examples of a nominal predictor, expressed in terms of Equations 14.5 and 14.6). The left panel shows a case for which J = 2, and the right panel shows a case in which J = 5. Notice that the deflections from baseline sum to zero, as\n",
    "demanded by the constraint in Equation 14.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"5.png\"><figcaption> \n",
    "</figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"6.png\"><figcaption> \n",
    "</figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 14.1.6.2 Additive combination of nominal predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Suppose we have two (or more) nominal predictors of a metric value. For example, we\n",
    "might be interested in predicting income as a function of political party affiliation and gender. Figure 14.4 showed examples of each of those predictors individually. What we do\n",
    "now is consider the joint influence of those predictors. If the two influences are merely\n",
    "additive, then the model from Equation 14.5 becomes\n",
    "(14.7)\n",
    "with the constraints\n",
    "(14.8)\n",
    "The left panel of Figure 14.5 shows an example of two nominal predictors that have\n",
    "additive effects on the predicted variable. In this case, the overall baseline is y = 6. When\n",
    "x\n",
    "1 = h1, 0i, there is a deflection in y of −1, and when x1 = h0, 1i, there is a deflection in y\n",
    "of +1. This deflection by x1 is the same at every level of x2. The deflections for the three\n",
    "levels of x\n",
    "2 are +3, −2, and −1. These deflections are the same at all levels of x1 . Formally,\n",
    "the left panel of Figure 14.5 is expressed mathematically by the additive combination:\n",
    "y = 6 + h−1, 1i−→x 1 + h3, −2, −1i−→x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 14.1.6.3 Nonadditive interaction of nominal predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When the predictor variables are non-metric, it does not even make sense to talk about a\n",
    "multiplicative interaction, because there are no numerical values to multiply. For example,\n",
    "consider predicting annual income from political party affiliation and gender. Both predictors are nominal, so it makes no sense to “multiply” them. But it does make sense to\n",
    "consider non-additive combination of their influences.2\n",
    "For example, the overall influence of gender is that men, on average, have a higher\n",
    "income than women. The overall influence of political party affiliation is that Republicans,\n",
    "on average, have higher income than Democrats. But it may be that the influences combine\n",
    "non-additively: Perhaps people who are both Republican and male have a higher average\n",
    "income than would be predicted by merely adding the average income boosts for being\n",
    "Republican and for being male. (This interaction is not claimed to be true; it is being used\n",
    "only as a hypothetical example.)\n",
    "We need new notation to formalize the non-additive influence of a combination of nominal values. Just as −→x 1 refers to the value of predictor 1, and −→x 2 refers to the value of\n",
    "predictor 2, the notation −→x 1×2 will refer to a particular combination of values of predictors 1 and 2. If there are J1 levels of predictor 1 and J2 levels of predictor 2, then there are\n",
    "J1 × J2 combinations of the two predictors.\n",
    "A non-additive interaction of predictors is formally represented by including a term\n",
    "for the influence of combinations of predictors, beyond the additive influences, as follows:\n",
    "y = β0 +\n",
    "−→\n",
    "β 1−→x 1 + −→β 2−→x 2 + −→β 1×2−→x 1×2. Whenever the interaction coefficient −→β 1×2 is non-zero,\n",
    "the predicted value of y is not a mere addition of the separate influences of the predictors.\n",
    "The right panel of Figure 14.5 shows a graphical example of two nominal predictors\n",
    "that have interactive (i.e., non-additive) effects on the predicted variable. Notice, in the left\n",
    "pair of bars (x2 = h1, 0, 0i), that a change from x1 = h1, 0i to x1 = h0, 1i produces an\n",
    "increase of +2 in y, from y = 8 to y = 10. But for the middle pair of bars (x2 = h0, 1, 0i), a\n",
    "change from x1 = h1, 0i to x1 = h0, 1i produces an increase of −2 in y, from y = 5 to y = 3.\n",
    "Thus, the influence of x1 is not the same at all levels of x2.\n",
    "An interesting aspect of the pattern in the right panel of Figure 14.5 is that the average\n",
    "influences of x1 and x2 are the same as in the left panel. Overall, on average, going from\n",
    "x1 = h1, 0i to x1 = h0, 1i produces a change of+2 in y, in both the left and right panels. And\n",
    "overall, on average, for both panels it is the case that x2 = h1, 0, 0i is +3 above baseline,\n",
    "x2 = h0, 1, 0i is −2 below baseline, and x2 = h0, 0, 1i is −1 below baseline. The only\n",
    "difference between the two panels is that the combined influence of the two predictors\n",
    "equals the sum of the individual influences in the left panel, but the combined influence of\n",
    "the two predictors does not equal the sum of the individual influences in the right panel.\n",
    "An interaction between nominal predictors consists of a distinct deflection, for each\n",
    "specific combination of categorical values, away from the additive combination. The magnitude of the interactive deflection is whatever is left over after the additive effects have\n",
    "been applied to the baseline. The model that includes an interaction term can be written as\n",
    "(14.9)\n",
    "with the constraints\n",
    "(14.10)\n",
    "In these equations, the term −→x 1×2 has J1 times J2 components, all of which are zero except\n",
    "for a 1 at the particular combination of levels of x1 and x2. This mysterious and arcane\n",
    "notation will be revealed in all its majestic grandeur in Chapter 19. For now, the main\n",
    "point is to understand that the term “interaction” refers to a non-additive influence of the\n",
    "predictors on the predicted, regardless of whether the predictors are measured on a nominal\n",
    "scale or a metric scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.7 Linking combined predictors to the predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Once the predictor variables are combined, they need to be mapped to the predicted variable. This mathematical mapping is called the (inverse) link function, denoted by f() in the\n",
    "following equation:\n",
    "(14.11)\n",
    "Until now, we have been assuming that the link function is merely the identity function,\n",
    "f(x) = x. For example, in Equation 14.9, y equals the linear combination of the predictors;\n",
    "there is no transformation of the linear combination before mapping the result to y.\n",
    "Before describing different link functions, it is important to make some clarifications\n",
    "of terminology and corresponding concepts. First, the function f() in Equation 14.11 is\n",
    "usually called the inverse link function, because the link function itself is thought of as\n",
    "transforming the value y into a form that can be linked to the linear model. I will abuse\n",
    "convention and simply refer to either f() or f−1 () as “the” link function, and rely on context\n",
    "to disambiguate which direction of linkage is intended. The reason for this terminological\n",
    "sadism is that the arrows in hierarchical diagrams of Bayesian models will flow from the\n",
    "linear model toward the data, and therefore it is natural for the functions to map toward the\n",
    "data, as in Equation 14.11. But repeatedly referring to this function as the “inverse” link\n",
    "would strain my patience and violate my aesthetic sensibilities. Second, the value y that\n",
    "results from the link function f(x) is not a data value per se. Instead, f(x) is the value of\n",
    "a parameter that expresses some characteristic of the data, usually their mean. Therefore\n",
    "the function f() in Equation 14.11 is sometimes called the mean function, and is written\n",
    "µ = f() instead of y = f(). I will not use this terminology because most students already\n",
    "think that “mean” means something else, namely the sum divided by N. The fact that y in\n",
    "Equation 14.11 is a parameter value and not a data value will become clear in subsequent\n",
    "sections as we encounter specific cases and examples.\n",
    "There are situations in which a non-identity link function is appropriate. Consider,\n",
    "for example, predicting response time as a function of amount of caffeine consumed. Response time declines as caffeine dosage increases, and therefore a linear prediction of RT\n",
    "from dosage would have a negative slope. This negative slope implies that for a very large\n",
    "dosage of caffeine, response time would become negative, which is impossible unless caffeine causes precognition (i.e., foreseeing events before they occur). Therefore a direct linear function cannot be used for extrapolation to large doses, and we might instead want\n",
    "to use an exponential link function such as y = exp \u0010β0 + β1 x\u0011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"7.png\"><figcaption> \n",
    "</figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 14.1.7.1 The sigmoid (a.k.a. logistic) function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A frequently used link function is the sigmoid , also known as the logistic:\n",
    "y = sig(x) = 1 \u000e1 + exp( −x)\u0001 (14.12)\n",
    "Notice the negative sign in front of the x. The sigmoid function ranges between 0 and 1.\n",
    "The sigmoid is nearly 0 when x is large negative, and is nearly 1 when x is large positive.\n",
    "For linear combinations of predictors, the sigmoid link function is most conveniently\n",
    "parameterized in x threshold form. For a single predictor variable, the sigmoid link function\n",
    "applied to the linear function of the predictor yields\n",
    "y = sig (x; γ, θ) = 1 \u000e1 + exp ( −γ (x − θ))\u0001 (14.13)\n",
    "where γ, called the gain, corresponds to β1 in Equation 14.2, and where θ, called the threshold, corresponds to −β0/β1 in Equation 14.2.\n",
    "Examples of Equation 14.13, i.e., the sigmoid of a single predictor, are shown in Figure 14.6. Notice that the threshold is the point on the x axis for which y = 0.5. The gain\n",
    "indicates how steeply the sigmoid rises through that point.\n",
    "Figure 14.7 shows examples of a sigmoid of two predictor variables. Above each panel\n",
    "is the equation for the corresponding graph. The equations are parameterized in x threshold\n",
    "form, as in Equation 14.4. In other words, y = sig \u0010γ \u0010Pk wkxk − θ\u0011\u0011, with \u0010Pk w2k\u00111/2 = 1.\n",
    "Notice, in particular, that the coefficients of x1 and x2 in the plotted equations do indeed have\n",
    "Euclidean length of 1.0. For example, in the upper-right panel, \u00100.71 2 + 0.71 2\u00111/2 = 1.0,\n",
    "except for rounding error.\n",
    "The coefficients of the x variables determine the orientation of the sigmoidal “cliff”. For\n",
    "example, compare the two top panels in Figure 14.7, which differ only in the coefficients, not in gain or threshold. In the top left panel, the coefficients are w1 = 0 and w2 = 1, and\n",
    "the cliff rises in the x2 direction. In the top right panel, the coefficients are w1 = 0.71 and\n",
    "w2 = 0.71, and the cliff rises in the positive diagonal direction.\n",
    "The threshold determines the position of the sigmoidal cliff. In other words, the threshold determines the x values for which y = 0.5. For example, compare the two left panels\n",
    "of Figure 14.7. The coefficients are the same, but the thresholds (and gains) are different.\n",
    "In the upper left panel, the threshold is zero, and therefore the mid-level of the cliff is over\n",
    "x2 = 0. In the lower left panel, the threshold is −3, and therefore the mid-level of the cliff\n",
    "is over x2 = −3.\n",
    "The gain determines the steepness of the sigmoidal cliff. Again compare the two left\n",
    "panels of Figure 14.7. The gain of the upper left is 1, whereas the gain of the lower left is 2.\n",
    "Terminology: The logit function. The inverse of the logistic function is called the\n",
    "logit function. For 0 < p < 1, logit(p) = log (p/(1 − p)). It is easy to show (try it!)\n",
    "that logit(sig(x)) = x, which is to say that the logit is indeed the inverse of the sigmoid.\n",
    "Some authors, and programmers, prefer to express the connection between predictors and\n",
    "predicted in the opposite direction, by first transforming the predicted variable to match the linear model. In other words, you may see the link expressed either of these ways:\n",
    "y = logistic \u0010β0 + β1 x1 + . . .\u0011\n",
    "logit(y) = β0 + β1 x1 + . . .\n",
    "The two expressions achieve the same result, mathematically. The difference between them\n",
    "is merely a matter of emphasis. In the first expression, the combination of predictors is\n",
    "transformed so it maps onto y expressed in its original scale. In the second expression, y is\n",
    "transformed onto a new scale, and that transformed value is modeled as a combination of\n",
    "predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"8.png\"><figcaption> \n",
    "</figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"9.png\"><figcaption> \n",
    "</figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 14.1.7.2 The cumulative normal (a.k.a. Phi) function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Another frequently used link function is the cumulative normal distribution. It is qualitatively very similar to the sigmoid or logistic function. Modelers will use the logistic or\n",
    "the cumulative normal depending on mathematical convenience or ease of interpretation.\n",
    "For example, when we consider ordinal predicted variables (in Chapter 21), it will be natural to model the responses in terms of a continuous underlying variable that has normally\n",
    "distributed variability, which leads to using the cumulative normal as a model of response\n",
    "probabilities.\n",
    "The cumulative normal is denoted Φ(x, µ, τ), where x is a real number and where µ\n",
    "and τ are parameter values, called the mean and precision of the normal distribution. The\n",
    "parameter µ governs the point at which the cumulative normal, Φ(x), equals 0.5. In other\n",
    "words, µ plays the same role as the threshold in the logistic sigmoid. The parameter τ\n",
    "governs the steepness of the cumulative normal function at x = µ. The τ parameter plays the same role as the gain parameter in the logistic sigmoid. A graph of a cumulative normal\n",
    "appears in Figure 14.8. For this example, µ = 0, and notice that Φ(0) = 0.5. This means\n",
    "that the area under the normal density to the left of 0 is 0.5.\n",
    "Terminology: The probit function. The inverse of the cumulative normal is called the\n",
    "probit function. (“Probit” stands for “probability unit”; Bliss, 1934). The probit function\n",
    "maps a value p, for 0.0 ≤ p ≤ 1.0, onto the infinite real line, and a graph of the probit\n",
    "function looks very much like the logit function. You may see the link expressed either of\n",
    "these ways: Traditionally, the transformation of y (in this case, the probit function) is called the link\n",
    "function, and the transformation of the linear combination of x (in this case, the Φ function)\n",
    "is called the inverse link function. As mentioned before, I abuse the traditional terminology\n",
    "and call either one a link function, relying on context to disambiguate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.8 Probabilistic prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the real world, there is always variation in y that we cannot predict from x. This unpredictable “noise” in y might be deterministically caused by sundry factors we have neither\n",
    "measured nor controlled, or the noise might be caused by inherent non-determinism in y. It\n",
    "does not matter either way because in practice the best we can do is predict the probability\n",
    "that y will have any particular value, dependent upon x. Therefore we use the deterministic\n",
    "value predicted by Equation 14.11 as the predicted tendency of y as a function of the predictors. We do not predict that y is exactly f\u0010β0 + β1 x1 + β2 x2 + β1×2 x1×2\u0011 because we would\n",
    "surely be wrong. Instead, we predict that y tends to be near f\u0010β0 + β1 x1 + β2 x2 + β1×2 x1×2\u0011.\n",
    "To make this notion of probabilistic tendency precise, we need to specify a probability\n",
    "distribution for y that depends on f\u0010β0 + β1 x1 + β2 x2 + β1×2 x1×2\u0011. To keep the notation\n",
    "tractable, first define µ = f\u0010β0 + β1 x1 + β2 x2 + β1×2 x1×2\u0011. Do not confuse this use of µ\n",
    "with the unrelated µ mentioned in the cumulative normal function. With this notation, we\n",
    "then denote the probability distribution of y as some to-be-specified probability density\n",
    "function, abbreviated as “pdf”:\n",
    "y ∼ pdf(µ [, τ, ...])\n",
    "The pdf might have various additional parameters, denoted by τ, ..., to specify its shape.\n",
    "Examples are provided in the next section, where all these ideas are brought together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.9 Formal expression of the GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the likelihood function specifies the probability of each possible predicted value\n",
    "y as a function of the predictor values x j and various parameter values β, τ etc. The generalized linear model can be written:\n",
    "(14,14)\n",
    "15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"10.png\"><figcaption> \n",
    "</figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"11.png\"><figcaption> \n",
    "</figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function f in Equation 14.14 is called the “link” function, because it links the combination of predictors to the predicted tendency. The optional parameters [, τ, ...] in Equation 14.15 may be needed for various types of the probability density function (pdf) that\n",
    "describe the probability distribution of y around the tendency µ.\n",
    "Figure 14.9 shows a random sample of points normally distributed around a line or\n",
    "plane. The upper panel illustrates a case of the generalized linear model of Equations 14.14\n",
    "and 14.15 in which there is a single predictor x, with β0 = 10 and β1 = 2. The link function\n",
    "is simply the identity function, f(β0 + β1 x) = β0 + β1 x. The probability density function is\n",
    "normal with a standard deviation of 2.0. Profiles of this normal density are superimposed\n",
    "on the graph to make it explicit. Notice that the normal density is always centered on the\n",
    "line that marks the predicted tendency as a function of the predictor.\n",
    "The lower panel of Figure 14.9 shows a case with two predictor variables. The predictors are combined linearly, with no interaction. The link function is the identity. The\n",
    "probability function is normal with a standard deviation of 4. Each randomly generated\n",
    "point is connected to the underlying linear core by a vertical dotted line, to explicitly indicate the random variation of the point from the plane. The plane marks the predicted\n",
    "tendency as a function of the predictors, and the data values are normally distributed above\n",
    "and below that tendency.\n",
    "Figure 14.10 shows another case of the GLM. In this case, the points are Bernoulli\n",
    "distributed around a sigmoid function of two predictors, as annotated at the top of the graph.\n",
    "There is a linear combination of predictors, with a sigmoid link function, and a Bernoulli\n",
    "probability function that defines the probability that y = 1. The graph shows that values of y\n",
    "can only be 0 or 1, and the sigmoid function defines the probability that y is 1 for particular predictor values. The sigmoidal surface plots the tendency that y = 1 as a function of the\n",
    "predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2 Cases of the GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 14.1, p. 312, displays the various cases of the generalized linear model that are considered in this book. Subsequent chapters of the book progress through the table in reading\n",
    "order: left to right within rows, then top to bottom across rows.\n",
    "The first row of Table 14.1 lists cases for which the predicted variable is metric. Moving\n",
    "from left to right within this row, the first column indicates a situation in which there is only\n",
    "a single group, and the predicted value for the group is simply the mean of the group. In\n",
    "this situation, there is no need to explicitly denote a predictor variable, and instead the mean\n",
    "of the group can be denoted by a single parameter, β0. This situation corresponds to what\n",
    "classical null hypothesis significance testing (NHST) calls a single-group t-test. This case\n",
    "is described in its Bayesian setting in Chapter 15.\n",
    "Moving to the next column, there is a single metric predictor. This corresponds to socalled “simple linear regression”, and is explored in Chapter 16. By inspecting the equation\n",
    "for the GLM in the cell, you can see that the only difference from the previous cell is the\n",
    "inclusion of the predictor x1 and its coefficient β1 .\n",
    "Moving rightward to the next column, we come to the scenario involving two or more\n",
    "metric predictors, which corresponds to “multiple regression”, and is explored in Chapter 17. By examining the equations for the GLM in the cell, you can see that the basic form\n",
    "is the same, but merely with extra terms added for the additional predictors.\n",
    "The next two columns involve nominal predictors, instead of metric predictors, with the\n",
    "penultimate column devoted to a single predictor and the final column devoted to two or\n",
    "more predictors. The last two columns correspond to what NHST calls “oneway ANOVA”\n",
    "and “multifactor ANOVA”. If that terminology is unfamiliar to you, don’t worry, it will be\n",
    "explained in Chapters 18 and 19.\n",
    "In all the cases in the first row, the link function is the identity, and the probability distribution for the metric predicted values in assumed to be normal. When we move to the\n",
    "second row, however, the predicted variable is dichotomous, and therefore the probability\n",
    "distribution for y is a Bernoulli distribution. The link function, which connects the predictors to the probability that y = 1, is assumed to be the sigmoid, i.e., logistic function. When\n",
    "the predictors are metric, this situation is generically referred to as “logistic regression” and\n",
    "is discussed in Chapter 20. The case of nominal predictors is also discussed.\n",
    "Finally, the bottom row of Table 14.1 lists cases for which the predicted variable is\n",
    "ordinal. These cases are considered in Chapter 21. Notice that the link function is the\n",
    "cumulative normal instead of the sigmoid, and the ordinal values are generated by multiplecategory generalization of the Bernoulli function, denoted by dcat. Again, this will be\n",
    "explained at length in the forthcoming chapters. The point here is for you to see the overall\n",
    "organization of topics, and to see how all these cases are variations of the same underlying\n",
    "structure.\n",
    "The table can be expanded with additional rows and columns, but then it gets too big\n",
    "to display easily. Additional columns would include combinations of metric and nominal\n",
    "predictors. But it turns out that it is easy in Bayesian models to combine metric and nominal predictors, once you know how to handle metric and nominal predictors individually.\n",
    "Additional rows would involve different types of predicted variables. In particular, a fourth row would include count data for the predicted values. We will, in fact, cover one such\n",
    "case, as described in the next section. When the predicted data are count values, a natural\n",
    "link function is the exponential, and a natural pdf is the Poisson distribution, which will\n",
    "be defined later in the book (Section 22.1.3). In summary, the rows of the table refer to\n",
    "differently scaled predicted values, with their corresponding link functions and pdf’s:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<figure id=\"fig.redline0\" style=\"float: none\"><img src=\"12.png\"><figcaption> \n",
    "</figcaption></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2.1 Two or more nominal variables predicting frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will also consider the situation in which there are two or more nominal variables\n",
    "used as predictors of a frequency count. A frequency count, i.e., how many times something\n",
    "happened, is a special case of a metric scale, but because its values fall at discrete levels,\n",
    "namely non-negative integers, this situation will have a different sort of likelihood distribution. This type of situation, with nominal predictors and frequency-count predicted values,\n",
    "is often called “contingency table analysis” and a typical NHST analysis conducts a “chisquare test of independence of attributes”. We explore Bayesian analysis of this situation in\n",
    "Chapter 22.\n",
    "Here is a brief summary of how contingency tables are analyzed using a model much\n",
    "like those in Table 14.1. In fact, a fourth row could be added to Table 14.1, with the predicted type labeled frequency count, and the model falling in the final column, under two\n",
    "nominal predictors. As a concrete example, suppose we measure political affiliation and\n",
    "religious affiliation of a set of people, and for a sample of people we count how many\n",
    "occurrences there are of each combination. We are interested in analyzing possible relationships between political and religious affiliations. Suppose we conduct a poll for one\n",
    "week. We happen to record 27 people who are Democrats and Unitarians. This observed\n",
    "frequency reflects an underlying rate at which that combination is generated by this sort\n",
    "of poll, i.e., the underlying rate for Unitarian Democrats is roughly 27 people per week.\n",
    "The observed rate (i.e., frequency per unit time) for each combination of nominal values\n",
    "is thought to reflect the true underlying rate at which that combination is generated by the\n",
    "world. We conceive of the observed rate as being a random sample from a true underlying\n",
    "rate denoted by λ. The probability of any particular observed rate, given an underlying\n",
    "rate of λ, is modeled by a Poisson distribution, which is denoted as freq ∼ dpois(λ). The\n",
    "Poisson distribution was smuggled into the text back in Exercise 11.3, p. 235, which I’m\n",
    "sure is still as fresh in your memory as a beached fish. Don’t worry, the Poisson will be\n",
    "explained again later (Section 22.1.3). The Poisson distribution specifies a probability for\n",
    "each possible observed rate. The Poisson puts highest probabilities on rates near λ.\n",
    "Our goal is to estimate the underlying rates at which each nominal combination is produced. But more than that, we would like to know if the attributes occur independently of\n",
    "each other, or instead covary in some way. For example, if political and religious affiliation\n",
    "are independent, then there should be the same proportion of Unitarians among Democrats\n",
    "as among Republicans. Mathematically, independence means p(Unitarian&Democrat) =\n",
    "p(Unitarian) × p(Democrat) and p(Unitarian&Republican) = p(Unitarian) × p(Republican)\n",
    "and so on for every combination of attribute values. To shorten the expressions, I’ll substi tute U for Unitarian and D for Democrat, whereby independence means\n",
    "p(U&D) = p(U) × p(D)\n",
    "and so on for every combination of attributes. That expression for probabilities corresponds\n",
    "to the following expression in terms of frequencies:\n",
    "freq(U&D)/N = freq(U)/N × freq(D)/N\n",
    "which can be re-arranged as\n",
    "freq(U&D) = freq(U) × freq(D) × 1/N.\n",
    "Notice that independence is expressed as a multiplicative product of attribute influences.\n",
    "But all the models we’ve considered in this chapter used an additive sum of predictor influences. To be able to use our familiar additive models, we’ll transform the frequencies by a\n",
    "logarithm, because the logarithm of the product of values equals the sum of the logarithms\n",
    "of the values. In other words,\n",
    "log(freq(U&D)) = log(freq(U)) + log(freq(D)) + log(1/N).\n",
    "The notation “log(freq(value))” gets cumbersome, so we’ll substitute the notation βv. Thus,\n",
    "log(freq(U&D)) = βU + βD + β0,\n",
    "where β0 stands in for the constant log(1/N). Finally, it’s unintuitive to talk about the\n",
    "logarithms of frequencies, so we’ll exponentiate to get rid of the leading logarithm, yielding:\n",
    "freq(U&D) = exp (βU + βD + β0) .\n",
    "To summarize, if independence is true, then the expression above should be true, for every\n",
    "combination of attribute values.\n",
    "But of course the attributes are usually not independent, and we would like some measure of lack of independence. We already have such a measure in the context of linear\n",
    "models, namely, the interaction term. Thus, we will include an interaction term that estimates deviation from independence. Thus, our model ends up being as follows. For two\n",
    "nominal attributes, we put the observed frequencies in a table, with one attribute’s values\n",
    "listed down the rows, and the other attribute’s values listed across the columns. The frequency in the rth row and cth column is denoted freqrc, and the underlying rate for that cell\n",
    "is denoted λ\n",
    "rc. The model looks like this:\n",
    "λ\n",
    "rc = exp (β0 + βr + βc + βr ×c)\n",
    "freqrc ∼ dpois(λrc) (14.16)\n",
    "with the usual constraints (from Equation 14.10)\n",
    "Xr\n",
    "βr = 0 and X\n",
    "c\n",
    "βc = 0 and X\n",
    "r\n",
    "βr ×c,r,c = 0 ∀c and X\n",
    "c\n",
    "βr×c,r,c = 0 ∀r\n",
    "The point of this over-fast prelude to contingency table analysis is merely to demonstrate that the core of the model we’ll be using is the same as the linear model that was\n",
    "mentioned for multifactor ANOVA in Table 14.1. Thus, all the applied analyses we’ll see\n",
    "in the remainder of the book are based on the GLM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
